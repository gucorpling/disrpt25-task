# -*- coding: utf-8 -*-
"""mt5_small_LCDF_embed.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NtDTqV2csTwwQblZDKNGTMrUHM_hNNzA
"""

!pip install torch transformers datasets
!pip install numpy==1.26.4

from google.colab import userdata
import os
os.environ['GIT_TOKEN'] = secret_value = userdata.get('GIT_TOKEN')

!git clone https://$GIT_TOKEN@github.com/Abhishek-P/disrpt25-task.git

# Commented out IPython magic to ensure Python compatibility.
# %cd '/content/disrpt25-task'
!ls
!pip install conllu

from google.colab import drive
drive.mount('/content/drive/', force_remount=True)
import sys
sys.path.append('/content/disrpt25-task')

from transformers import MT5Tokenizer, MT5ForConditionalGeneration
import torch
import torch.nn as nn
import datasets
from transformers import Trainer, TrainingArguments, DataCollatorWithPadding
from sklearn.metrics import classification_report, accuracy_score, f1_score
import argparse
import numpy as np
import csv

import disrptdata
disrptdata.DATA_DIR = "/content/drive/MyDrive/Colab Notebooks/sample_data"

# === load dataset ===
"""
experiment with two languages: Chinese and English
"""
DATA_DIR = "/content/drive/MyDrive/Colab Notebooks/sample_data"

# load and combine datasets
zho = disrptdata.get_dataset("zho.rst.gcdt")
eng = disrptdata.get_dataset("eng.erst.gum")
combined = disrptdata.get_combined_dataset()
print("Train examples:", combined["train"].num_rows)
print("Dev examples:", combined["dev"].num_rows)

train_dataset = combined['train']
dev_dataset = combined['dev']
train_dataset = train_dataset.class_encode_column('label')
dev_dataset   = dev_dataset.class_encode_column('label')

tokenizer = MT5Tokenizer.from_pretrained("google/mt5-base")

# mappings
lang2id = {'eng': 0, 'zho': 1}
fw2id = {'rst': 0, 'erst': 1}
corpus2id = {'gum': 0, 'gcdt': 1}
dir2id = {'1>2': 0, '1<2': 1}


# preprocess data
def preprocess(example):
    text = f"Classify: Arg1: {example['u1']} Arg2: {example['u2']}"
    encoded = tokenizer(text, padding="max_length", truncation=True, max_length=512)
    encoded["label"] = example["label"]

    # add meta info ids
    encoded["language_ids"] = lang2id[example["lang"]]
    encoded["framework_ids"] = fw2id[example["framework"]]
    encoded["corpus_ids"] = corpus2id[example["corpus"]]
    encoded["direction_ids"] = dir2id[example["direction"]]

    return encoded

# will use batch when dataset size scales up
train_tokenized = train_dataset.map(preprocess, batched=False)
dev_tokenized   = dev_dataset.map(preprocess, batched=False)

train_tokenized.set_format('torch', columns=[
    "input_ids", "attention_mask", "label",
    "language_ids", "framework_ids", "corpus_ids", "direction_ids"
])
dev_tokenized.set_format('torch', columns=[
    "input_ids", "attention_mask", "label",
    "language_ids", "framework_ids", "corpus_ids", "direction_ids"
])

class MT5Classifier(nn.Module):
    def __init__(self, num_labels, num_languages=2, num_frameworks=2, num_corpora=2, lang_emb_dim=None):
        super().__init__()
        self.encoder = MT5ForConditionalGeneration.from_pretrained("google/mt5-small").get_encoder()
        hidden_size = self.encoder.config.d_model

        # embedding layer for language, corpus, direction, framework
        self.language_embedding = nn.Embedding(num_languages, hidden_size)
        self.framework_embedding = nn.Embedding(num_frameworks, hidden_size)
        self.corpus_embedding = nn.Embedding(num_corpora, hidden_size)
        self.direction_embedding = nn.Embedding(2, hidden_size)  # 0 = 1>2, 1 = 1<2

        # classifier head
        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Linear(hidden_size, num_labels)
        self.loss_fct = nn.CrossEntropyLoss()


    def forward(self, input_ids, attention_mask,
            language_ids=None, framework_ids=None, corpus_ids=None, direction_ids=None,
            labels=None):

        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
        pooled = outputs.last_hidden_state.mean(dim=1)  # shape: [batch_size, hidden_size]

        # add metadata embeddings
        if language_ids is not None:
          pooled += self.language_embedding(language_ids)

        if framework_ids is not None:
          pooled += self.framework_embedding(framework_ids)

        if corpus_ids is not None:
          pooled += self.corpus_embedding(corpus_ids)

        if direction_ids is not None:
          pooled += self.direction_embedding(direction_ids)

        logits = self.classifier(pooled)
        loss = self.loss_fct(logits, labels) if labels is not None else None
        return {"loss": loss, "logits": logits}

num_labels = train_tokenized.features["label"].num_classes
num_languages = len(lang2id)
num_frameworks = len(fw2id)
num_corpora = len(corpus2id)

model = MT5Classifier(
    num_labels=num_labels,
    num_languages=num_languages,
    num_frameworks=num_frameworks,
    num_corpora=num_corpora,
    lang_emb_dim=None
)

# === training setup ===
use_cuda = True
device = torch.device("cuda" if torch.cuda.is_available() and use_cuda else "cpu")

training_args = TrainingArguments(
    output_dir="/content/drive/MyDrive/mt5_embedding/mt5_classifier_embedding.results",
    overwrite_output_dir=False,
    learning_rate=2e-5,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    num_train_epochs=2,
    weight_decay=0.01,
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    auto_find_batch_size=True,
)


data_collator = DataCollatorWithPadding(tokenizer)

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=1)
    acc = accuracy_score(labels, preds)
    f1 = f1_score(labels, preds, average="weighted")

    # get label names from the feature
    label_names = train_tokenized.features["label"].names

    report = classification_report(labels, preds, target_names=label_names)
    print("\n=== Classification Report ===")
    print(report)
    return {"accuracy": acc, "f1": f1}


trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_tokenized,
    eval_dataset=dev_tokenized,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

batch = next(iter(trainer.get_train_dataloader()))
print("Batch input_ids shape:", batch["input_ids"].shape)

trainer.train()
trainer.evaluate()

# === error analysis ===
# get predictions on dev set
pred_out = trainer.predict(dev_tokenized)
logits = pred_out.predictions
labels = pred_out.label_ids

# convert logits to label IDs
preds = np.argmax(logits, axis=1)

# softmax confidence score
probs = torch.softmax(torch.tensor(logits), dim=1).numpy()

# extract missclassified examples
dev_texts = dev_tokenized
formatted_texts = [
    f"Arg1: {u1} | Arg2: {u2}"
    for u1, u2 in zip(dev_texts["u1"], dev_texts["u2"])
]

mis = [
    (text, label_encoder.classes_[true], label_encoder.classes_[pred], probs[i][pred])
    for i, (text, true, pred) in enumerate(zip(formatted_texts, labels, preds))
    if true != pred
]

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

out_path = '/content/drive/MyDrive/mt5_vanilla/misclassifications_metainfo.csv'

# log the results to a csv file
with open(out_path, 'w', newline='', encoding='utf-8') as f:
    writer = csv.writer(f)
    writer.writerow(["text", "true_label", "pred_label", "confidence"])
    writer.writerows(mis)

print(f"âœ… Saved {len(mis)} misclassified examples to:\n{out_path}")