# -*- coding: utf-8 -*-
"""mt5_small_LCFD_tokens.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XF7lrYCsvf7OS5rxOEC12TC-nu0bVhO9
"""

!pip install torch transformers datasets
!pip install numpy==1.26.4

from google.colab import userdata
import os
os.environ['GIT_TOKEN'] = secret_value = userdata.get('GIT_TOKEN')

!git clone https://$GIT_TOKEN@github.com/Abhishek-P/disrpt25-task.git

# Commented out IPython magic to ensure Python compatibility.
# %cd '/content/disrpt25-task'
!ls
!pip install conllu

from google.colab import drive
drive.mount('/content/drive/', force_remount=True)
import sys
sys.path.append('/content/disrpt25-task')

from transformers import MT5Tokenizer, MT5ForConditionalGeneration
import torch
import torch.nn as nn
import datasets
from transformers import Trainer, TrainingArguments, DataCollatorWithPadding
from sklearn.metrics import classification_report, accuracy_score, f1_score
import argparse
import numpy as np
import csv

import disrptdata
disrptdata.DATA_DIR = "/content/drive/MyDrive/Colab Notebooks/sample_data"

# === load dataset ===
"""
experiment with two languages: Chinese and English
"""
DATA_DIR = "/content/drive/MyDrive/Colab Notebooks/sample_data"

# load and combine datasets
zho = disrptdata.get_dataset("zho.rst.gcdt")
eng = disrptdata.get_dataset("eng.erst.gum")
combined = disrptdata.get_combined_dataset()
print("Train examples:", combined["train"].num_rows)
print("Dev examples:", combined["dev"].num_rows)

train_dataset = combined['train']
dev_dataset = combined['dev']
train_dataset = train_dataset.class_encode_column('label')
dev_dataset   = dev_dataset.class_encode_column('label')

tokenizer = MT5Tokenizer.from_pretrained("google/mt5-base")

# preprocess data
def preprocess(example):
    # encode language, framework, corpus name, relation direction as tokens to the input
    # ref: https://gitlab.irit.fr/melodi/andiamo/relation-classification-features/-/blob/main/utils.py?ref_type=heads

    meta = (f"[LANG:{example['lang']}] "
            f"[FW:{example['framework']}] "
            f"[CORP:{example['corpus']}]")

    if example['direction'] == '1>2':
        span = f"Arg1: }} {example['u1']} > [SEP] Arg2: {example['u2']}"
    elif example['direction'] == '1<2':
        span = f"Arg1: {example['u1']} < Arg2: {example['u2']} {{"
    else:
        span = f"Arg1: {example['u1']} [SEP] Arg2: {example['u2']}"

    text = f"Classify: {meta} {span}"

    # text = f"Classify: {meta} Arg1: {example['u1']} Arg2: {example['u2']}"
    encoded = tokenizer(text, padding="max_length", truncation=True, max_length=512)
    encoded["label"] = example["label"]
    return encoded

# will use batch when dataset size scales up
train_tokenized = train_dataset.map(preprocess, batched=False)
dev_tokenized   = dev_dataset.map(preprocess, batched=False)

train_tokenized.set_format('torch', columns=["input_ids", "attention_mask", "label"])
dev_tokenized.set_format('torch', columns=["input_ids", "attention_mask", "label"])

class MT5Classifier(nn.Module):
    def __init__(self, num_labels):
        super().__init__()
        self.encoder = MT5ForConditionalGeneration.from_pretrained("google/mt5-small").get_encoder()
        hidden_size = self.encoder.config.d_model
        # classifier head
        self.classifier = nn.Linear(hidden_size, num_labels)
        self.loss_fct = nn.CrossEntropyLoss()

    def forward(self, input_ids, attention_mask, labels=None, **kwargs):
        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
        pooled = outputs.last_hidden_state.mean(dim=1)
        logits = self.classifier(pooled)
        loss = self.loss_fct(logits, labels) if labels is not None else None
        return {"loss": loss, "logits": logits}

num_labels = train_tokenized.features["label"].num_classes
model = MT5Classifier(num_labels=num_labels)

# === training setup ===
use_cuda = True
device = torch.device("cuda" if torch.cuda.is_available() and use_cuda else "cpu")

training_args = TrainingArguments(
    output_dir="/content/drive/MyDrive/mt5_LCFD/mt5_classifier_LCFD_2.results",
    overwrite_output_dir=False,
    learning_rate=2e-5,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    num_train_epochs=2,
    weight_decay=0.01,
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    auto_find_batch_size=True,
)


data_collator = DataCollatorWithPadding(tokenizer)

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=1)
    acc = accuracy_score(labels, preds)
    f1 = f1_score(labels, preds, average="weighted")

    # get label names from the feature
    label_names = train_tokenized.features["label"].names

    report = classification_report(labels, preds, target_names=label_names)
    print("\n=== Classification Report ===")
    print(report)
    return {"accuracy": acc, "f1": f1}


trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_tokenized,
    eval_dataset=dev_tokenized,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

batch = next(iter(trainer.get_train_dataloader()))
print("Batch input_ids shape:", batch["input_ids"].shape)

trainer.train()
trainer.evaluate()

# === error analysis ===
import csv
import torch
import numpy as np
from sklearn.metrics import classification_report, accuracy_score, f1_score

# Get predictions on tokenized dev set
pred_out = trainer.predict(dev_tokenized)
logits = pred_out.predictions
labels = pred_out.label_ids
preds = np.argmax(logits, axis=1)
probs = torch.softmax(torch.tensor(logits), dim=1).numpy()

# Use label names from the dataset
label_names = dev_dataset.features["label"].names

# Use original dev_dataset (not tokenized) for u1 and u2
formatted_texts = [
    f"Arg1: {ex['u1']} | Arg2: {ex['u2']}" for ex in dev_dataset
]

# Extract misclassified examples
mis = [
    (text, label_names[true], label_names[pred], probs[i][pred])
    for i, (text, true, pred) in enumerate(zip(formatted_texts, labels, preds))
    if true != pred
]

# Save to Drive
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

out_path = "/content/drive/MyDrive/mt5_vanilla/misclassifications_LCFD.csv"

with open(out_path, "w", newline="", encoding="utf-8") as f:
    writer = csv.writer(f)
    writer.writerow(["text", "true_label", "pred_label", "confidence"])
    writer.writerows(mis)

print(f"âœ… Saved {len(mis)} misclassified examples to:\n{out_path}")