# -*- coding: utf-8 -*-
"""mt5_small_LCFD_tokens.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XF7lrYCsvf7OS5rxOEC12TC-nu0bVhO9
"""

!pip install torch transformers datasets
!pip install numpy==1.26.4

from google.colab import userdata
import os
os.environ['GIT_TOKEN'] = secret_value = userdata.get('GIT_TOKEN')

!git clone https://$GIT_TOKEN@github.com/Abhishek-P/disrpt25-task.git

# Commented out IPython magic to ensure Python compatibility.
# %cd '/content/disrpt25-task'
!ls
!pip install conllu

from google.colab import drive
drive.mount('/content/drive/', force_remount=True)
import sys
sys.path.append('/content/disrpt25-task')

from transformers import MT5Tokenizer, MT5ForConditionalGeneration
import torch
import torch.nn as nn
import datasets
from transformers import Trainer, TrainingArguments, DataCollatorWithPadding
from sklearn.metrics import classification_report, accuracy_score, f1_score
import argparse
import numpy as np
import csv

import disrptdata
disrptdata.DATA_DIR = "/content/drive/MyDrive/Colab Notebooks/sample_data"

# === load dataset ===
"""
experiment with two languages: Chinese and English
"""
DATA_DIR = "/content/drive/MyDrive/Colab Notebooks/sample_data"

# load and combine datasets
zho = disrptdata.get_dataset("zho.rst.gcdt")
eng = disrptdata.get_dataset("eng.erst.gum")
combined = disrptdata.get_combined_dataset()
print("Train examples:", combined["train"].num_rows)
print("Dev examples:", combined["dev"].num_rows)

train_dataset = combined['train']
dev_dataset = combined['dev']
train_dataset = train_dataset.class_encode_column('label')
dev_dataset   = dev_dataset.class_encode_column('label')

tokenizer = MT5Tokenizer.from_pretrained("google/mt5-base")

# add features as tokens
# ref: https://gitlab.irit.fr/melodi/andiamo/relation-classification-features/-/blob/main/utils.py?ref_type=heads
langs = set(combined['train']['lang'])
frameworks = set(combined['train']['framework'])
corpora = set(combined['train']['corpus'])

lang_tokens = [f"[LANG:{l}]" for l in langs]
framework_tokens = [f"[FW:{f}]" for f in frameworks]
corpus_tokens = [f"[CORP:{c}]" for c in corpora]

meta_tokens = lang_tokens + framework_tokens + corpus_tokens
tokenizer.add_tokens(meta_tokens)

def preprocess(example):
    # encode LCF features (language, corpus, framework)
    meta = (f"[LANG:{example['lang']}] "
            f"[FW:{example['framework']}] "
            f"[CORP:{example['corpus']}]")

    # encode directions
    if example['direction'] == '1>2':
        span = f"Arg1: }} {example['u1']} > [SEP] Arg2: {example['u2']}"
    elif example['direction'] == '1<2':
        span = f"Arg1: {example['u1']} < Arg2: {example['u2']} {{"
    else:
        span = f"Arg1: {example['u1']} [SEP] Arg2: {example['u2']}"

    text = f"Classify: {meta} {span}"

    # text = f"Classify: {meta} Arg1: {example['u1']} Arg2: {example['u2']}"
    encoded = tokenizer(text, padding="max_length", truncation=True, max_length=512)
    encoded["label"] = example["label"]
    return encoded


train_tokenized = train_dataset.map(preprocess, batched=True)
dev_tokenized   = dev_dataset.map(preprocess, batched=True)

train_tokenized.set_format('torch', columns=["input_ids", "attention_mask", "label"])
dev_tokenized.set_format('torch', columns=["input_ids", "attention_mask", "label"])

# KeyError                                  Traceback (most recent call last)
# /tmp/ipython-input-10-2934672637.py in <cell line: 0>()
#       6 
#       7 # load and combine datasets
# ----> 8 zho = disrptdata.get_dataset("zho.rst.gcdt")
#       9 eng = disrptdata.get_dataset("eng.erst.gum")
#      10 combined = disrptdata.get_combined_dataset()

# 8 frames
# /content/disrpt25-task/disrptdata.py in get_dataset(dataset_name, context_size)
#      28 def get_dataset(dataset_name, context_size=-1):
#      29     language, framework, corpus = get_meta_features_for_dataset(dataset_name)
# ---> 30     return load_training_dataset(dataset_name, language, framework, corpus, context_size=context_size)
#      31 
#      32 # Read the .conllu files and convert them to a dataset

# /content/disrpt25-task/disrptdata.py in load_training_dataset(dataset_name, lang, framework, corpus, context_size)
#     205 
#     206     load_split_if_it_exists("dev")
# --> 207     load_split_if_it_exists("train")
#     208 
#     209     # Augment the dataset with meta features

# /content/disrpt25-task/disrptdata.py in load_split_if_it_exists(split_name, context_size)
#     194             rels = read_rels_split(f"{DATA_DIR}/{dataset_name}/{dataset_name}_{split_name}", lang, framework, corpus)
#     195             spans, tokens = get_segs_and_toks_for_docs(f"{DATA_DIR}/{dataset_name}/{dataset_name}_{split_name}", with_segs=True)
# --> 196             rels = rels.map(
#     197                 lambda x: {
#     198                     "doc_tokens": tokens[x["doc_id"]],

# /usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py in wrapper(*args, **kwargs)
#     590             self: "Dataset" = kwargs.pop("self")
#     591         # apply actual function
# --> 592         out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
#     593         datasets: List["Dataset"] = list(out.values()) if isinstance(out, dict) else [out]
#     594         for dataset in datasets:

# /usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py in wrapper(*args, **kwargs)
#     555         }
#     556         # apply actual function
# --> 557         out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
#     558         datasets: List["Dataset"] = list(out.values()) if isinstance(out, dict) else [out]
#     559         # re-apply format to the output

# /usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py in map(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)
#    3095                     desc=desc or "Map",
#    3096                 ) as pbar:
# -> 3097                     for rank, done, content in Dataset._map_single(**dataset_kwargs):
#    3098                         if done:
#    3099                             shards_done += 1

# /usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py in _map_single(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)
#    3448                     _time = time.time()
#    3449                     for i, example in shard_iterable:
# -> 3450                         example = apply_function_on_filtered_inputs(example, i, offset=offset)
#    3451                         if update_data:
#    3452                             if i == 0:

# /usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py in apply_function_on_filtered_inputs(pa_inputs, indices, check_same_num_examples, offset)
#    3351             if with_rank:
#    3352                 additional_args += (rank,)
# -> 3353             processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
#    3354             if isinstance(processed_inputs, LazyDict):
#    3355                 processed_inputs = {

# /content/disrpt25-task/disrptdata.py in <lambda>(x)
#     196             rels = rels.map(
#     197                 lambda x: {
# --> 198                     "doc_tokens": tokens[x["doc_id"]],
#     199                 }
#     200             )

# KeyError: 'gcdt_academic_dingzhen'

class MT5Classifier(nn.Module):
    def __init__(self, num_labels):
        super().__init__()
        self.encoder = MT5ForConditionalGeneration.from_pretrained("google/mt5-small").get_encoder()
        hidden_size = self.encoder.config.d_model
        # classifier head
        self.classifier = nn.Linear(hidden_size, num_labels)
        self.loss_fct = nn.CrossEntropyLoss()

    def forward(self, input_ids, attention_mask, labels=None, **kwargs):
        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
        pooled = outputs.last_hidden_state.mean(dim=1)
        logits = self.classifier(pooled)
        loss = self.loss_fct(logits, labels) if labels is not None else None
        return {"loss": loss, "logits": logits}

num_labels = train_tokenized.features["label"].num_classes
model = MT5Classifier(num_labels=num_labels)
model.resize_token_embeddings(len(tokenizer))

# === training setup ===
use_cuda = True
device = torch.device("cuda" if torch.cuda.is_available() and use_cuda else "cpu")

training_args = TrainingArguments(
    output_dir="/content/drive/MyDrive/mt5_LCFD/mt5_classifier_LCFD_3.results",
    overwrite_output_dir=False,
    learning_rate=2e-5,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    num_train_epochs=2,
    weight_decay=0.01,
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    auto_find_batch_size=True,
)


data_collator = DataCollatorWithPadding(tokenizer)

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=1)
    acc = accuracy_score(labels, preds)
    f1 = f1_score(labels, preds, average="weighted")

    # get label names from the feature
    label_names = train_tokenized.features["label"].names

    report = classification_report(labels, preds, target_names=label_names)
    print("\n=== Classification Report ===")
    print(report)
    return {"accuracy": acc, "f1": f1}


trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_tokenized,
    eval_dataset=dev_tokenized,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

batch = next(iter(trainer.get_train_dataloader()))
print("Batch input_ids shape:", batch["input_ids"].shape)

trainer.train()
trainer.evaluate()

# === error analysis ===
import csv
import torch
import numpy as np
from sklearn.metrics import classification_report, accuracy_score, f1_score

# Get predictions on tokenized dev set
pred_out = trainer.predict(dev_tokenized)
logits = pred_out.predictions
labels = pred_out.label_ids
preds = np.argmax(logits, axis=1)
probs = torch.softmax(torch.tensor(logits), dim=1).numpy()

# Use label names from the dataset
label_names = dev_dataset.features["label"].names

# Use original dev_dataset (not tokenized) for u1 and u2
formatted_texts = [
    f"Arg1: {ex['u1']} | Arg2: {ex['u2']}" for ex in dev_dataset
]

# Extract misclassified examples
mis = [
    (text, label_names[true], label_names[pred], probs[i][pred])
    for i, (text, true, pred) in enumerate(zip(formatted_texts, labels, preds))
    if true != pred
]

# Save to Drive
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

out_path = "/content/drive/MyDrive/mt5_vanilla/misclassifications_LCFD.csv"

with open(out_path, "w", newline="", encoding="utf-8") as f:
    writer = csv.writer(f)
    writer.writerow(["text", "true_label", "pred_label", "confidence"])
    writer.writerows(mis)

print(f"âœ… Saved {len(mis)} misclassified examples to:\n{out_path}")